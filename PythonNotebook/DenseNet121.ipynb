{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSs0dWRQTw-9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 87] The parameter is incorrect: 'c:\\\\Users\\\\A8Z7LZZ\\\\OneDrive - 3M\\\\Documents\\\\GP\\\\BT5153_GP'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras import Sequential\n",
        "import efficientnet.keras as efn\n",
        "import numpy as np\n",
        "import math\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, f1_score, precision_score, recall_score, precision_recall_curve\n",
        "import seaborn as sns\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "rTCUOHk6feht",
        "outputId": "39de6e7d-12c2-4140-8cca-931713d37b37"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def count_images_per_class(folder_path):\n",
        "    class_counts = {}\n",
        "    for class_name in sorted(os.listdir(folder_path)):\n",
        "        class_dir = os.path.join(folder_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            count = len([\n",
        "                f for f in os.listdir(class_dir)\n",
        "                if os.path.isfile(os.path.join(class_dir, f))\n",
        "            ])\n",
        "            class_counts[class_name] = count\n",
        "    return class_counts\n",
        "\n",
        "# Count images in each split\n",
        "train_counts = count_images_per_class('split_dataset/train')\n",
        "val_counts = count_images_per_class('split_dataset/val')\n",
        "test_counts = count_images_per_class('split_dataset/test')\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame({\n",
        "    'train': train_counts,\n",
        "    'val': val_counts,\n",
        "    'test': test_counts\n",
        "}).fillna(0).astype(int).T\n",
        "\n",
        "summary_df = summary_df.T  # Transpose to get classes as rows\n",
        "summary_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbY6xgBVgLkK"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "INPUT_SHAPE = (224, 224, 3)\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNvaM7zTgjZr",
        "outputId": "dca07625-9467-496f-9744-4b73f12bb429"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    rescale=1. / 255)\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'split_dataset/train',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    'split_dataset/val',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    'split_dataset/test',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCguoHRQiB8k"
      },
      "outputs": [],
      "source": [
        "# Create base DenseNet model\n",
        "base_model = DenseNet121(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3),\n",
        "    pooling='avg'  # Global Average Pooling for output (1024,)\n",
        ")\n",
        "base_model.trainable = False  # Freeze base model initially\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=base_model.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt0bcOoJhmBQ",
        "outputId": "d446744a-360b-4280-83d0-e5bf592843a3"
      },
      "outputs": [],
      "source": [
        "\"\"\"nb_eval_samples = len(train_generator.filenames)\n",
        "num_classes = len(train_generator.class_indices)\n",
        "predict_size = int(math.ceil(nb_eval_samples / BATCH_SIZE))\n",
        "\n",
        "bottleneck_features = model.predict(train_generator, verbose=1)\n",
        "print(bottleneck_features.shape)\n",
        "\n",
        "nb_eval_samples_val = len(val_generator.filenames)\n",
        "num_classes_val = len(val_generator.class_indices)\n",
        "predict_size_val = int(math.ceil(nb_eval_samples_val / BATCH_SIZE))\n",
        "\n",
        "bottleneck_features_val = model.predict(val_generator, verbose=1)\n",
        "print(bottleneck_features_val.shape)\n",
        "\n",
        "nb_eval_samples_test = len(test_generator.filenames)\n",
        "num_classes_test = len(test_generator.class_indices)\n",
        "predict_size_test = int(math.ceil(nb_eval_samples_test / BATCH_SIZE))\n",
        "\n",
        "bottleneck_features_test = model.predict(test_generator, verbose=1)\n",
        "print(bottleneck_features_test.shape)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeW2UTNuiJdu",
        "outputId": "006d75ad-d9a5-484d-f39b-8c034146ac12"
      },
      "outputs": [],
      "source": [
        "\"\"\"if np.max(train_generator.classes) == 5:\n",
        "    y = np.array(train_generator.classes) - 1\n",
        "else:\n",
        "    y = np.array(train_generator.classes)\n",
        "\n",
        "np.save('split_dataset/5153_classes_dn121.npy', y)\n",
        "\n",
        "\n",
        "print(bottleneck_features.shape)\n",
        "np.save('split_dataset/5153_features_dn121.npy', bottleneck_features)\n",
        "\n",
        "if np.max(val_generator.classes) == 5:\n",
        "    y = np.array(val_generator.classes) - 1\n",
        "else:\n",
        "    y = np.array(val_generator.classes)\n",
        "\n",
        "np.save('split_dataset/5153_classes_dn121_val.npy', y)\n",
        "\n",
        "\n",
        "print(bottleneck_features_val.shape)\n",
        "np.save('split_dataset/5153_features_dn121_val.npy', bottleneck_features_val)\n",
        "\n",
        "if np.max(test_generator.classes) == 5:\n",
        "    y = np.array(test_generator.classes) - 1\n",
        "else:\n",
        "    y = np.array(test_generator.classes)\n",
        "\n",
        "np.save('split_dataset/5153_classes_dn121_test.npy', y)\n",
        "\n",
        "\n",
        "print(bottleneck_features_test.shape)\n",
        "np.save('split_dataset/5153_features_dn121_test.npy', bottleneck_features_test)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS8F5IvKj022",
        "outputId": "2154cb03-30c7-442b-db50-0359cec821ca"
      },
      "outputs": [],
      "source": [
        "X_train = np.load('split_dataset/5153_features_dn121.npy')\n",
        "y_train = np.load('split_dataset/5153_classes_dn121.npy')\n",
        "\n",
        "print(\"Train Features shape:\", X_train.shape)  # e.g., (num_samples, 7, 7, 1280)\n",
        "print(\"Train Labels shape:\", y_train.shape)\n",
        "\n",
        "X_val = np.load('split_dataset/5153_features_dn121_val.npy')\n",
        "y_val = np.load('split_dataset/5153_classes_dn121_val.npy')\n",
        "\n",
        "print(\"Val Features shape:\", X_val.shape)  # e.g., (num_samples, 7, 7, 1280)\n",
        "print(\"Val Labels shape:\", y_val.shape)\n",
        "\n",
        "X_test = np.load('split_dataset/5153_features_dn121_test.npy')\n",
        "y_test = np.load('split_dataset/5153_classes_dn121_test.npy')\n",
        "\n",
        "print(\"Test Features shape:\", X_test.shape)  # e.g., (num_samples, 7, 7, 1280)\n",
        "print(\"Test Labels shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoNeHJ5rkMVq"
      },
      "outputs": [],
      "source": [
        "# One-hot encode labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Assuming y_train and y_val are 1D integer class labels (0 to 4)\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes=5)\n",
        "y_test_cat = to_categorical(y_test, num_classes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure your feature extractor outputs the correct dimensions\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
        "print(f\"Feature extractor output shape: {feature_extractor.output_shape}\")  # Should print (None, 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EVY_f3cqv7"
      },
      "source": [
        "Build & Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PP_xeNoVcqv7",
        "outputId": "946b8769-3ead-453f-bc55-5456097cc26a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from tensorflow.keras.activations import gelu\n",
        "\n",
        "# Example: Different alphas for classes (e.g., [0.25, 0.75, 1.0, ...])\n",
        "alpha_per_class = tf.constant([0.25, 0.7, 1.0, 0.8, 0.9], dtype=tf.float32)\n",
        "\n",
        "def focal_loss_with_smoothing_and_dynamic_alpha(alpha_per_class, gamma=2.0, smoothing=0.1):\n",
        "    alpha_per_class = tf.constant(alpha_per_class, dtype=tf.float32)\n",
        "    \n",
        "    def loss_fn(y_true, y_pred):\n",
        "        num_classes = tf.cast(tf.shape(y_true)[-1], dtype=tf.float32)\n",
        "        y_true = y_true * (1.0 - smoothing) + smoothing / num_classes\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        focal_weight = tf.pow(1. - y_pred, gamma)\n",
        "        \n",
        "        # Apply alpha weight per sample based on true class\n",
        "        alpha_weight = tf.reduce_sum(alpha_per_class * y_true, axis=-1, keepdims=True)\n",
        "        loss = focal_weight * cross_entropy * alpha_weight\n",
        "\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "    \n",
        "    return loss_fn\n",
        "\n",
        "def confidence_penalty_loss(base_loss_fn, beta=0.01):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        base_loss = base_loss_fn(y_true, y_pred)\n",
        "        \n",
        "        # Entropy: -sum(p * log(p))\n",
        "        entropy = -tf.reduce_sum(y_pred * tf.math.log(y_pred + K.epsilon()), axis=-1)\n",
        "        \n",
        "        return base_loss + beta * entropy\n",
        "    return loss_fn\n",
        "\n",
        "# Define class-wise alpha weights (e.g. based on inverse class frequency)\n",
        "alpha_per_class = [1.0, 2.0, 1.5, 0.7, 1.2]  # Example for 5 classes â€” tune as needed\n",
        "\n",
        "# Create focal loss with smoothing and dynamic alpha\n",
        "base_loss = focal_loss_with_smoothing_and_dynamic_alpha(alpha_per_class, gamma=2.0, smoothing=0.1)\n",
        "\n",
        "# Wrap it with confidence penalty\n",
        "final_loss = confidence_penalty_loss(base_loss, beta=0.01)\n",
        "\n",
        "# COMPUTE CLASS WEIGHTS\n",
        "y_train_int = np.argmax(y_train_cat, axis=1)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_int), y=y_train_int)\n",
        "class_weights = class_weights / np.max(class_weights) * 3.0\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Increase weight for class 1 (moderate)\n",
        "class_weight_dict[1] = class_weight_dict[1] * 1.6  # Boost moderate class\n",
        "class_weight_dict[3] = class_weight_dict[3] * 1.4  # Higher boost for proliferative class\n",
        "class_weight_dict[4] = class_weight_dict[4] * 1.2  # Higher boost for severe class\n",
        "\n",
        "print(\"Modified class weights:\", class_weight_dict)\n",
        "\n",
        "class KappaHistory(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        super().__init__()\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.kappa_train = []\n",
        "        self.kappa_val = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Predict class labels for train and val sets\n",
        "        y_train_pred = np.argmax(self.model.predict(self.X_train, verbose=0), axis=1)\n",
        "        y_val_pred = np.argmax(self.model.predict(self.X_val, verbose=0), axis=1)\n",
        "        \n",
        "        # For the labels, if they are one-hot encoded, you can use argmax to get the class index\n",
        "        y_train_true = np.argmax(self.y_train, axis=1) if len(self.y_train.shape) > 1 else self.y_train\n",
        "        y_val_true = np.argmax(self.y_val, axis=1) if len(self.y_val.shape) > 1 else self.y_val\n",
        "\n",
        "        # Calculate Kappa scores\n",
        "        train_kappa = cohen_kappa_score(y_train_true, y_train_pred)\n",
        "        val_kappa = cohen_kappa_score(y_val_true, y_val_pred)\n",
        "\n",
        "        # Append kappa values\n",
        "        self.kappa_train.append(train_kappa)\n",
        "        self.kappa_val.append(val_kappa)\n",
        "\n",
        "        # Print the Kappa score for the current epoch\n",
        "        print(f\"\\nEpoch {epoch + 1} - Train Kappa: {train_kappa:.4f}, Val Kappa: {val_kappa:.4f}\")\n",
        "\n",
        "        # Optionally add kappa to logs for later visualization or saving\n",
        "        if logs is not None:\n",
        "            logs['train_kappa'] = train_kappa\n",
        "            logs['val_kappa'] = val_kappa\n",
        "\n",
        "# Instantiate the KappaHistory callback\n",
        "kappa_callback = KappaHistory(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Define model using Sequential API\n",
        "feature_dim = 1024  # DenseNet121 with global average pooling outputs 1024 features\n",
        "model = models.Sequential([\n",
        "    layers.InputLayer(input_shape=(feature_dim,)),\n",
        "    # Deeper network\n",
        "    #layers.GaussianNoise(0.1),\n",
        "    layers.Dense(256, activation=tf.nn.swish, kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    layers.BatchNormalization(momentum=0.95),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(128, activation=tf.nn.swish, kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    layers.BatchNormalization(momentum=0.95),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation=tf.nn.swish, kernel_regularizer=regularizers.l2(1e-2)),\n",
        "    layers.BatchNormalization(momentum=0.95),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\"\"\"# Use Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-4  # Some versions support weight_decay directly\n",
        ")\"\"\"\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(\n",
        "    learning_rate=1e-3,\n",
        "    rho=0.9,              # Decay factor for moving average of squared gradients\n",
        "    momentum=0.0,         # Optional momentum term\n",
        "    epsilon=1e-7,         # Smoothing term to avoid division by zero\n",
        "    centered=False        # If True, gradients are normalized by estimated variance\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=final_loss,\n",
        "    metrics=['accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc', multi_label=True, num_labels=num_classes)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "output_dir = 'split_dataset/output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    os.path.join(output_dir, 'best_model.h5'),\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Cosine annealing learning rate schedule\n",
        "import math\n",
        "def cosine_annealing(epoch, total_epochs=50, initial_lr=3e-4, min_lr=1e-5):\n",
        "    return min_lr + (initial_lr - min_lr) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(\n",
        "    lambda epoch: cosine_annealing(epoch, total_epochs=30, initial_lr=1e-3, min_lr=1e-5)\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[checkpoint, early_stopping, lr_scheduler, kappa_callback],\n",
        "    class_weight=class_weight_dict  # Apply class weights\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('Model AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Kappa\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(kappa_callback.kappa_train)\n",
        "plt.plot(kappa_callback.kappa_val)\n",
        "plt.title(\"Model Kappa\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Kappa\")\n",
        "plt.legend([\"Train\", \"Validation\"], loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After initial head training is complete:\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "class KappaHistory(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        super().__init__()\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.kappa_train = []\n",
        "        self.kappa_val = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Predict class labels for train and val sets\n",
        "        y_train_pred = np.argmax(self.model.predict(self.X_train, verbose=0), axis=1)\n",
        "        y_val_pred = np.argmax(self.model.predict(self.X_val, verbose=0), axis=1)\n",
        "        \n",
        "        # For the labels, if they are one-hot encoded, you can use argmax to get the class index\n",
        "        y_train_true = np.argmax(self.y_train, axis=1) if len(self.y_train.shape) > 1 else self.y_train\n",
        "        y_val_true = np.argmax(self.y_val, axis=1) if len(self.y_val.shape) > 1 else self.y_val\n",
        "\n",
        "        # Calculate Kappa scores\n",
        "        train_kappa = cohen_kappa_score(y_train_true, y_train_pred)\n",
        "        val_kappa = cohen_kappa_score(y_val_true, y_val_pred)\n",
        "\n",
        "        # Append kappa values\n",
        "        self.kappa_train.append(train_kappa)\n",
        "        self.kappa_val.append(val_kappa)\n",
        "\n",
        "        # Print the Kappa score for the current epoch\n",
        "        print(f\"\\nEpoch {epoch + 1} - Train Kappa: {train_kappa:.4f}, Val Kappa: {val_kappa:.4f}\")\n",
        "\n",
        "        # Optionally add kappa to logs for later visualization or saving\n",
        "        if logs is not None:\n",
        "            logs['train_kappa'] = train_kappa\n",
        "            logs['val_kappa'] = val_kappa\n",
        "\n",
        "# Instantiate the KappaHistory callback\n",
        "kappa_callback = KappaHistory(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Set up ImageDataGenerator for DenseNet121 input\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    rescale=1. / 255,\n",
        "    preprocessing_function=tf.keras.applications.densenet.preprocess_input  # Preprocess for DenseNet121\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    preprocessing_function=tf.keras.applications.densenet.preprocess_input  # Preprocess for DenseNet121\n",
        ")\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'split_dataset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    'split_dataset/val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Instantiate the KappaHistory callback\n",
        "kappa_callback = KappaHistory(val_generator)\n",
        "\n",
        "# Create integrated model with Sequential API\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:  # Freeze all but the last 30 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the integrated model with Sequential API\n",
        "integrated_model = models.Sequential()\n",
        "integrated_model.add(base_model)\n",
        "integrated_model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(5e-5)))\n",
        "integrated_model.add(layers.BatchNormalization(momentum=0.95))\n",
        "integrated_model.add(layers.Dropout(0.3))\n",
        "integrated_model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
        "integrated_model.add(layers.BatchNormalization(momentum=0.95))\n",
        "integrated_model.add(layers.Dropout(0.2))\n",
        "integrated_model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(\n",
        "    learning_rate=1e-5,     # Lower learning rate for fine-tuning\n",
        "    rho=0.9,                # Decay factor for moving average of squared gradients\n",
        "    momentum=0.0,           # Optional momentum term\n",
        "    epsilon=1e-7,           # Smoothing term to avoid division by zero\n",
        "    centered=True           # Use centered gradients for stability\n",
        ")\n",
        "\n",
        "# Compile with lower learning rate for fine-tuning\n",
        "integrated_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=focal_loss_with_smoothing_and_dynamic_alpha(alpha_per_class, gamma=2.0, smoothing=0.1),\n",
        "    metrics=['accuracy', \n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc', multi_label=True, num_labels=num_classes)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fine-tune\n",
        "fine_tune_history = integrated_model.fit(\n",
        "    train_generator,  # Use image data, not features\n",
        "    validation_data=val_generator,\n",
        "    epochs=12,\n",
        "    batch_size=16,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint('fine_tuned_model.h5', monitor='val_loss', save_best_only=True),\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        LearningRateScheduler(lambda epoch: 1e-5 * (0.9 ** epoch)),\n",
        "        kappa_callback\n",
        "    ],\n",
        "    class_weight=class_weight_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot fine-tuning history\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(fine_tune_history.history['accuracy'])\n",
        "plt.plot(fine_tune_history.history['val_accuracy'])\n",
        "plt.title('Fine-tuning Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(fine_tune_history.history['loss'])\n",
        "plt.plot(fine_tune_history.history['val_loss'])\n",
        "plt.title('Fine-tuning Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(fine_tune_history.history['auc'])\n",
        "plt.plot(fine_tune_history.history['val_auc'])\n",
        "plt.title('Fine-tuning AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Kappa\n",
        "plt.subplot(1, 4, 4)\n",
        "#plt.plot(kappa_callback.kappa_train)\n",
        "plt.plot(kappa_callback.kappa_val)\n",
        "plt.title('Fine-tuning Kappa')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Kappa\")\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'fine_tuning_history.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7mK_lgf5fad"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pyHC_pdv5g7P",
        "outputId": "90a783ed-a605-48d1-a6b3-9826f083c32c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Ensure output dir exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Predict probabilities on validation set\n",
        "print(\"Running model inference on validation set...\")\n",
        "start_time = time.time()\n",
        "y_pred_probs = model.predict(X_val, verbose=1)\n",
        "end_time = time.time()\n",
        "\n",
        "# Predicted class labels\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Inference time\n",
        "total_samples = len(X_val)\n",
        "total_time_ms = (end_time - start_time) * 1000\n",
        "avg_inference_time = total_time_ms / total_samples\n",
        "\n",
        "# Compute Cohen's Kappa\n",
        "kappa_score = cohen_kappa_score(y_val, y_pred_classes)\n",
        "print(f\"\\nCohen's Kappa Score: {kappa_score:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "report_str = classification_report(y_val, y_pred_classes, digits=4)\n",
        "print(report_str)\n",
        "\n",
        "with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n",
        "    f.write(report_str)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_val, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Binarize true labels for ROC curve (needed for multi-class ROC)\n",
        "y_val_bin = label_binarize(y_val, classes=list(range(num_classes)))\n",
        "\n",
        "# ROC curve & AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_val_bin[:, i], y_pred_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Macro-average ROC\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "mean_tpr /= num_classes\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label=f\"Macro-average ROC curve (area = {roc_auc['macro']:.2f})\",\n",
        "         color='navy', linestyle='--', linewidth=2)\n",
        "\n",
        "colors = plt.cm.get_cmap('tab10', num_classes)\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors(i),\n",
        "             lw=2, label=f\"Class {i} (area = {roc_auc[i]:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves for Multi-class Classification\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(output_dir, 'roc_curves_multiclass.png'))\n",
        "plt.show()\n",
        "\n",
        "# Save summary metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "metrics_summary = {\n",
        "    \"accuracy\": accuracy_score(y_val, y_pred_classes),\n",
        "    \"macro_precision\": precision_score(y_val, y_pred_classes, average='macro'),\n",
        "    \"macro_recall\": recall_score(y_val, y_pred_classes, average='macro'),\n",
        "    \"macro_f1\": f1_score(y_val, y_pred_classes, average='macro'),\n",
        "    \"macro_roc_auc\": roc_auc[\"macro\"],\n",
        "    \"cohen_kappa\": kappa_score,  # <-- added here\n",
        "    \"avg_inference_time_ms\": avg_inference_time\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_summary.items(), columns=[\"Metric\", \"Value\"])\n",
        "metrics_df.to_csv(os.path.join(output_dir, 'evaluation_metrics.csv'), index=False)\n",
        "\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyoH0GdwTPu"
      },
      "source": [
        "Generate grad_CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QemdMRwegi84"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "filepaths = val_generator.filepaths\n",
        "\n",
        "def load_images_from_paths(paths, target_size=(224, 224)):\n",
        "    raw_imgs = []\n",
        "    preprocessed_imgs = []\n",
        "    for p in paths:\n",
        "        img = image.load_img(p, target_size=target_size)\n",
        "        arr = image.img_to_array(img)\n",
        "        raw_imgs.append(arr / 255.0)\n",
        "        preprocessed_imgs.append(preprocess_input(arr))\n",
        "    return np.array(preprocessed_imgs), raw_imgs\n",
        "\n",
        "X_val_processed, X_val_raw = load_images_from_paths(filepaths, target_size=IMAGE_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5BRfC48ljrG",
        "outputId": "f057fdba-de60-4211-cf9d-6faa393c4d08"
      },
      "outputs": [],
      "source": [
        "input_shape = model.input_shape\n",
        "print(\"Model expected input shape:\", input_shape)\n",
        "output_shape = model.output_shape\n",
        "print(\"Model output shape:\", output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find a suitable visualization layer - use a block layer to avoid shape issues\n",
        "last_conv_layer_name = None\n",
        "for layer_name in ['conv5_block16_concat', 'conv5_block15_concat', 'conv5_block14_concat']:\n",
        "    if any(layer.name == layer_name for layer in feature_extractor.layers):\n",
        "        last_conv_layer_name = layer_name\n",
        "        print(f\"Using layer for visualization: {last_conv_layer_name}\")\n",
        "        break\n",
        "\n",
        "# Create a model that goes up to the last conv layer\n",
        "activation_model = Model(\n",
        "    inputs=feature_extractor.input,\n",
        "    outputs=feature_extractor.get_layer(last_conv_layer_name).output\n",
        ")\n",
        "\n",
        "# Get validation data\n",
        "val_filepaths = ['split_dataset/val/' + fname for fname in val_generator.filenames[:5]]\n",
        "X_val_processed, X_val_raw = load_images_from_paths(val_filepaths, target_size=(224, 224))\n",
        "\n",
        "\n",
        "# Define class names\n",
        "class_names = ['Mild', 'Moderate', 'No DR', 'Proliferative DR', 'Severe']\n",
        "\n",
        "# Process validation images\n",
        "for idx, (input_img, raw_img, filepath) in enumerate(zip(X_val_processed[:5], X_val_raw[:5], val_filepaths[:5])):\n",
        "    print(f\"Processing image {idx} - {os.path.basename(filepath)}\")\n",
        "\n",
        "    # Create input tensor\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(input_img, axis=0), dtype=tf.float32)\n",
        "    \n",
        "    # Extract features for the classification model\n",
        "    features = feature_extractor(input_tensor)\n",
        "    print(f\"Feature shape: {features.shape}\")\n",
        "\n",
        "    # Get prediction\n",
        "    preds = model(features)\n",
        "    class_idx = np.argmax(preds[0])\n",
        "    confidence = preds[0][class_idx].numpy()\n",
        "    print(f\"Prediction: {class_names[class_idx]} with confidence {confidence:.4f}\")\n",
        "\n",
        "    # Compute Grad-CAM\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get conv output from the visualization layer\n",
        "        conv_output = activation_model(input_tensor)\n",
        "        tape.watch(conv_output)\n",
        "        \n",
        "        # Get features for classification by continuing through the rest of the model\n",
        "        remaining_layers = []\n",
        "        found_layer = False\n",
        "        for layer in feature_extractor.layers:\n",
        "            if found_layer:\n",
        "                remaining_layers.append(layer)\n",
        "            elif layer.name == last_conv_layer_name:\n",
        "                found_layer = True\n",
        "        \n",
        "        # Manual forward pass through remaining layers\n",
        "        x = conv_output\n",
        "        for layer in remaining_layers:\n",
        "            x = layer(x)\n",
        "        \n",
        "        # Use the classification model\n",
        "        pred = model(x)\n",
        "        \n",
        "        # Get the score for the predicted class\n",
        "        pred_class_idx = tf.argmax(pred[0])\n",
        "        score = pred[0, pred_class_idx]\n",
        "\n",
        "    # Get gradients of score with respect to conv_output\n",
        "    grads = tape.gradient(score, conv_output)\n",
        "    \n",
        "    if grads is not None:\n",
        "        # Global average pooling of gradients\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "        \n",
        "        # Weight the channels by the gradient importance\n",
        "        conv_output_value = conv_output[0].numpy()\n",
        "        heatmap = np.zeros(conv_output_value.shape[:-1])\n",
        "        \n",
        "        # Weight each channel in the feature map\n",
        "        for i, w in enumerate(pooled_grads):\n",
        "            heatmap += w.numpy() * conv_output_value[:, :, i]\n",
        "    else:\n",
        "        print(\"Warning: Gradients are None, using feature visualization\")\n",
        "        # Simple feature visualization\n",
        "        conv_output_value = conv_output[0].numpy()\n",
        "        heatmap = np.mean(conv_output_value, axis=-1)\n",
        "    \n",
        "    # ReLU and normalize\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap = heatmap / (np.max(heatmap) + 1e-10)\n",
        "    \n",
        "    # Resize to image size\n",
        "    heatmap = cv2.resize(heatmap, (raw_img.shape[1], raw_img.shape[0]))\n",
        "    \n",
        "    # Create colored heatmap\n",
        "    heatmap_colored = np.uint8(255 * heatmap)\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap_colored, cv2.COLORMAP_JET)\n",
        "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB) / 255.0\n",
        "    \n",
        "    # Superimpose on original image\n",
        "    superimposed = np.clip(0.7 * raw_img + 0.3 * heatmap_colored, 0, 1)\n",
        "    \n",
        "    # Save and display\n",
        "    class_label = class_names[class_idx]\n",
        "    filename = f\"{idx}_{class_label.replace(' ', '_')}.png\"\n",
        "    save_path = os.path.join(output_dir, filename)\n",
        "    plt.imsave(save_path, superimposed)\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(raw_img)\n",
        "    plt.title(f\"Original - {os.path.basename(filepath)}\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(superimposed)\n",
        "    plt.title(f\"Activation: {class_label} ({confidence:.2f})\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Saved visualization to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualization with ground truth labels\n",
        "print(\"\\nGenerating ground-truth based visualization grid...\")\n",
        "\n",
        "output_dir = 'split_dataset/output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Use DenseNet121\n",
        "feature_extractor = DenseNet121(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg',\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "feature_extractor.trainable = False\n",
        "\n",
        "# Find a suitable visualization layer\n",
        "last_conv_layer_name = None\n",
        "for layer_name in ['conv5_block16_concat', 'conv5_block15_concat', 'conv5_block14_concat']:\n",
        "    if any(layer.name == layer_name for layer in feature_extractor.layers):\n",
        "        last_conv_layer_name = layer_name\n",
        "        print(f\"Using layer for visualization: {last_conv_layer_name}\")\n",
        "        break\n",
        "\n",
        "# Create a model to get the activation maps\n",
        "activation_model = Model(\n",
        "    inputs=feature_extractor.input,\n",
        "    outputs=feature_extractor.get_layer(last_conv_layer_name).output\n",
        ")\n",
        "\n",
        "# Get validation data with ground truth labels\n",
        "all_val_filepaths = ['split_dataset/val/' + fname for fname in val_generator.filenames]\n",
        "\n",
        "# Extract ground truth labels from validation generator\n",
        "try:\n",
        "    true_classes = val_generator.classes\n",
        "    print(f\"Found {len(true_classes)} labeled samples\")\n",
        "except:\n",
        "    # If classes not available, check if folder structure indicates classes\n",
        "    true_classes = []\n",
        "    for path in all_val_filepaths:\n",
        "        # Extract class from folder structure - adjust as needed\n",
        "        parts = path.split('/')\n",
        "        if len(parts) >= 3:\n",
        "            class_folder = parts[-2]  # Assumes class name is the parent folder\n",
        "            try:\n",
        "                class_idx = class_names.index(class_folder)\n",
        "            except:\n",
        "                class_idx = int(class_folder) if class_folder.isdigit() else 0\n",
        "            true_classes.append(class_idx)\n",
        "    print(f\"Extracted {len(true_classes)} classes from folder structure\")\n",
        "\n",
        "# Create dictionary to store images by ground truth class\n",
        "images_by_class = {class_idx: [] for class_idx in range(len(class_names))}\n",
        "\n",
        "# Group images by their ground truth class\n",
        "for i, true_class in enumerate(true_classes):\n",
        "    if len(images_by_class[true_class]) < 5:  # Only collect up to 5 images per class\n",
        "        images_by_class[true_class].append(i)\n",
        "\n",
        "# Get predictions for all validation images\n",
        "all_preds = []\n",
        "batch_size = 32\n",
        "for i in range(0, len(all_val_filepaths), batch_size):\n",
        "    batch_paths = all_val_filepaths[i:i+batch_size]\n",
        "    batch_images, _ = load_images_from_paths(batch_paths, target_size=(224, 224))\n",
        "    batch_tensor = tf.convert_to_tensor(batch_images, dtype=tf.float32)\n",
        "    batch_features = feature_extractor(batch_tensor)\n",
        "    batch_preds = model(batch_features).numpy()\n",
        "    all_preds.extend(batch_preds)\n",
        "\n",
        "# Create visualization grid\n",
        "n_classes = len(class_names)\n",
        "n_examples = 5\n",
        "fig_width = 4 * n_examples\n",
        "fig_height = 3 * n_classes * 2  # 2 rows per class (original + heatmap)\n",
        "\n",
        "plt.figure(figsize=(fig_width, fig_height))\n",
        "\n",
        "for class_idx in range(n_classes):\n",
        "    # Get indices for this class\n",
        "    class_indices = images_by_class[class_idx]\n",
        "\n",
        "    # Check if we have images for this class\n",
        "    if not class_indices:\n",
        "        print(f\"No images found for class {class_names[class_idx]}\")\n",
        "        # Create empty subplot as placeholder\n",
        "        for col in range(1, n_examples+1):\n",
        "            plt.subplot(n_classes * 2, n_examples, (2 * class_idx) * n_examples + col)\n",
        "            plt.text(0.5, 0.5, f\"No {class_names[class_idx]} examples\",\n",
        "                    horizontalalignment='center', verticalalignment='center')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(n_classes * 2, n_examples, (2 * class_idx + 1) * n_examples + col)\n",
        "            plt.axis('off')\n",
        "        continue\n",
        "\n",
        "    # Load images for this class\n",
        "    class_paths = [all_val_filepaths[i] for i in class_indices]\n",
        "    class_images, class_raws = load_images_from_paths(class_paths, target_size=(224, 224))\n",
        "    class_preds = [all_preds[i] for i in class_indices]\n",
        "\n",
        "    # Display images for this class\n",
        "    for example_idx, (img_processed, img_raw, pred) in enumerate(zip(class_images, class_raws, class_preds)):\n",
        "        if example_idx >= n_examples:\n",
        "            break\n",
        "\n",
        "        # Get prediction details\n",
        "        pred_idx = np.argmax(pred)\n",
        "        pred_prob = pred[pred_idx]\n",
        "\n",
        "        # Create input tensor\n",
        "        input_tensor = tf.convert_to_tensor(np.expand_dims(img_processed, axis=0), dtype=tf.float32)\n",
        "\n",
        "        # Compute Grad-CAM\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Get conv output from the visualization layer\n",
        "            conv_output = activation_model(input_tensor)\n",
        "            tape.watch(conv_output)\n",
        "            \n",
        "            # Create a model from conv output to final features\n",
        "            remaining_layers = []\n",
        "            found_layer = False\n",
        "            for layer in feature_extractor.layers:\n",
        "                if found_layer:\n",
        "                    remaining_layers.append(layer)\n",
        "                elif layer.name == last_conv_layer_name:\n",
        "                    found_layer = True\n",
        "            \n",
        "            # Manual forward pass through remaining layers\n",
        "            x = conv_output\n",
        "            for layer in remaining_layers:\n",
        "                x = layer(x)\n",
        "            \n",
        "            # Get prediction from classifier\n",
        "            pred_tensor = model(x)\n",
        "            \n",
        "            # Get score for predicted class\n",
        "            score = pred_tensor[0, pred_idx]\n",
        "        \n",
        "        # Get gradients of score with respect to conv_output\n",
        "        grads = tape.gradient(score, conv_output)\n",
        "        \n",
        "        if grads is not None:\n",
        "            # Global average pooling of gradients\n",
        "            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "            \n",
        "            # Weight the channels by the gradient importance\n",
        "            conv_output_value = conv_output[0].numpy()\n",
        "            cam = np.zeros(conv_output_value.shape[:-1])\n",
        "            \n",
        "            # Weight each channel in the feature map\n",
        "            for i, w in enumerate(pooled_grads):\n",
        "                cam += w.numpy() * conv_output_value[:, :, i]\n",
        "        else:\n",
        "            # Fallback to simple feature visualization\n",
        "            print(f\"Warning: Gradients are None for class {class_names[class_idx]}, example {example_idx}\")\n",
        "            activations = activation_model(input_tensor)\n",
        "            cam = np.mean(activations[0].numpy(), axis=-1)\n",
        "        \n",
        "        # ReLU and normalize\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cam / np.max(cam) if np.max(cam) > 0 else cam\n",
        "        \n",
        "        # Resize to match image size\n",
        "        cam = cv2.resize(cam, (img_raw.shape[1], img_raw.shape[0]))\n",
        "        \n",
        "        # Create colored heatmap\n",
        "        heatmap = np.uint8(255 * cam)\n",
        "        heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "        heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB) / 255.0\n",
        "        \n",
        "        # Superimpose heatmap on original image\n",
        "        gradcam_img = np.clip(0.7 * img_raw + 0.3 * heatmap_color, 0, 1)\n",
        "        \n",
        "        # Calculate grid position\n",
        "        row = 2 * class_idx + 1  # Start at row 1, increment by 2 for each class\n",
        "        col = example_idx + 1\n",
        "        \n",
        "        # Display original image\n",
        "        plt.subplot(n_classes * 2, n_examples, (row - 1) * n_examples + col)\n",
        "        plt.imshow(img_raw)\n",
        "        \n",
        "        # Show both true and predicted labels\n",
        "        correct = pred_idx == class_idx\n",
        "        color = 'green' if correct else 'red'\n",
        "        title = f\"True: {class_names[class_idx]}\\nPred: {class_names[pred_idx]}\"\n",
        "        \n",
        "        if example_idx == 0:\n",
        "            title = f\"Class: {class_names[class_idx]}\\n{title}\"\n",
        "        \n",
        "        plt.title(title, color=color)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Display Grad-CAM\n",
        "        plt.subplot(n_classes * 2, n_examples, row * n_examples + col)\n",
        "        plt.imshow(gradcam_img)\n",
        "        plt.title(f\"Conf: {pred_prob:.2f}\", color=color)\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'dn121_gradcam.png'))\n",
        "plt.show()\n",
        "print(f\"Saved dn121_gradcam visualization to {os.path.join(output_dir, 'dn121_gradcam.png')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYDEUAL2wAMb"
      },
      "source": [
        "Predictions for Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIOWhJhkzyPj"
      },
      "outputs": [],
      "source": [
        "# One-hot encode labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming y_test are 1D integer class labels (0 to 4)\n",
        "y_test_cat = to_categorical(y_test, num_classes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LNSOoV9IwE8M",
        "outputId": "6530195a-90ed-4340-a23c-7a0e355d08df"
      },
      "outputs": [],
      "source": [
        "# Ensure output dir exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Predict probabilities on test set\n",
        "print(\"Running model inference on test set...\")\n",
        "start_time = time.time()\n",
        "y_pred_probs_test = model.predict(X_test, verbose=1)\n",
        "end_time = time.time()\n",
        "\n",
        "# Predicted class labels\n",
        "y_pred_classes_test = np.argmax(y_pred_probs_test, axis=1)\n",
        "\n",
        "# Inference time\n",
        "total_samples_test = len(X_test)\n",
        "total_time_ms_test = (end_time - start_time) * 1000\n",
        "avg_inference_time_test = total_time_ms_test / total_samples_test\n",
        "\n",
        "# Compute Cohen's Kappa for test set\n",
        "kappa_score_test = cohen_kappa_score(y_test, y_pred_classes_test)\n",
        "print(f\"\\nCohen's Kappa Score (Test Set): {kappa_score_test:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "report_str_test = classification_report(y_test, y_pred_classes_test, digits=4)\n",
        "print(report_str_test)\n",
        "\n",
        "with open(os.path.join(output_dir, 'classification_report_test.txt'), 'w') as f:\n",
        "    f.write(report_str_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm_test = confusion_matrix(y_test, y_pred_classes_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.savefig(os.path.join(output_dir, 'confusion_matrix_test.png'))\n",
        "plt.show()\n",
        "\n",
        "# Binarize true labels for ROC curve (needed for multi-class ROC)\n",
        "y_test_bin = label_binarize(y_test, classes=list(range(num_classes)))\n",
        "\n",
        "# ROC curve & AUC for each class\n",
        "fpr_test = dict()\n",
        "tpr_test = dict()\n",
        "roc_auc_test = dict()\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr_test[i], tpr_test[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs_test[:, i])\n",
        "    roc_auc_test[i] = auc(fpr_test[i], tpr_test[i])\n",
        "\n",
        "# Macro-average ROC\n",
        "all_fpr_test = np.unique(np.concatenate([fpr_test[i] for i in range(num_classes)]))\n",
        "mean_tpr_test = np.zeros_like(all_fpr_test)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    mean_tpr_test += np.interp(all_fpr_test, fpr_test[i], tpr_test[i])\n",
        "\n",
        "mean_tpr_test /= num_classes\n",
        "fpr_test[\"macro\"] = all_fpr_test\n",
        "tpr_test[\"macro\"] = mean_tpr_test\n",
        "roc_auc_test[\"macro\"] = auc(fpr_test[\"macro\"], tpr_test[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_test[\"macro\"], tpr_test[\"macro\"],\n",
        "         label=f\"Macro-average ROC curve (area = {roc_auc_test['macro']:.2f})\",\n",
        "         color='navy', linestyle='--', linewidth=2)\n",
        "\n",
        "colors = plt.cm.get_cmap('tab10', num_classes)\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr_test[i], tpr_test[i], color=colors(i),\n",
        "             lw=2, label=f\"Class {i} (area = {roc_auc_test[i]:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves for Multi-class Classification (Test Set)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(output_dir, 'roc_curves_multiclass_test.png'))\n",
        "plt.show()\n",
        "\n",
        "# Save summary metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "metrics_summary_test = {\n",
        "    \"accuracy\": accuracy_score(y_test, y_pred_classes_test),\n",
        "    \"macro_precision\": precision_score(y_test, y_pred_classes_test, average='macro'),\n",
        "    \"macro_recall\": recall_score(y_test, y_pred_classes_test, average='macro'),\n",
        "    \"macro_f1\": f1_score(y_test, y_pred_classes_test, average='macro'),\n",
        "    \"macro_roc_auc\": roc_auc_test[\"macro\"],\n",
        "    \"cohen_kappa\": kappa_score_test,  # <-- added here\n",
        "    \"avg_inference_time_ms\": avg_inference_time_test\n",
        "}\n",
        "metrics_df_test = pd.DataFrame(metrics_summary_test.items(), columns=[\"Metric\", \"Value\"])\n",
        "metrics_df_test.to_csv(os.path.join(output_dir, 'evaluation_metrics_test.csv'), index=False)\n",
        "\n",
        "print(\"\\nTest Set Evaluation Summary:\")\n",
        "print(metrics_df_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
